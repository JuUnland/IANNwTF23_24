{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import math"
      ],
      "metadata": {
        "id": "76WTYlPzuLzo"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "digits = load_digits()\n",
        "minibatchsize = 5\n",
        "target_size = len(digits.target_names)\n",
        "print(target_size)\n",
        "\n",
        "X = np.array(digits.data)\n",
        "y = np.array(digits.target)\n",
        "\n",
        "# scaling the numbers\n",
        "X = (X - np.min(X))/(np.max(X)-np.min(X))\n",
        "\n",
        "# float 64 shoudl be fine aswell\n",
        "print(X.dtype)\n",
        "\n",
        "# encoding into one hot vectors\n",
        "y = np.eye(target_size)[y]\n",
        "\n",
        "# helper function to confirm correct splitting\n",
        "def show_img(digits):\n",
        "  plt.gray()\n",
        "  plt.matshow(digits.reshape(8, 8))\n",
        "  plt.show()\n",
        "\n",
        "# yield X, y randomly\n",
        "def shuffle_generator(X, y):\n",
        "  shuffler = np.random.permutation(len(X))\n",
        "  X = X[shuffler]\n",
        "  y = y[shuffler]\n",
        "  idx = 0\n",
        "  while idx < len(X):\n",
        "    yield(X[idx], y[idx])\n",
        "    idx += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2y5RTM9uL-3",
        "outputId": "4a0e3412-7026-419c-d3fe-ba924de4b283"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ACTIVATION FUNCTIONS\n",
        "def sigmoid(input):\n",
        "  return 1/(1 + np.exp(-input))\n",
        "\n",
        "def sigmoid_backwards(input):\n",
        "  return sigmoid(input) * (1 - sigmoid(input))\n",
        "\n",
        "def softmax(input):\n",
        "  sum = np.sum(np.exp(input))\n",
        "  out = np.exp(input) / sum\n",
        "  return out\n",
        "\n",
        "\n",
        "# LOSS FUNCTION\n",
        "def cross_entropy(predictions, targets):\n",
        "  return -np.sum(targets * np.log(predictions + 10**-100))\n",
        "\n",
        "def cross_entropy_backwards(predictions, targets):\n",
        "  return predictions - targets\n"
      ],
      "metadata": {
        "id": "VCiwxUPv8PHk"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "class MLP_layer():\n",
        "  def __init__(self,units, input_size, activation_function, activation_function_backwards = 0):\n",
        "    self.activation_function = activation_function\n",
        "    self.activation_function_backwards = activation_function_backwards\n",
        "    self.units = units\n",
        "    self.input_size = input_size\n",
        "\n",
        "    self.input = np.zeros(input_size)\n",
        "    self.preactivation = np.zeros(units)\n",
        "    self.activation = np.zeros(units)\n",
        "    self.weights = np.random.normal(0, 0.2, size=(units, input_size))\n",
        "    self.bias = np.zeros(units)\n",
        "\n",
        "  def forward(self, input : np.ndarray):\n",
        "    self.input = input\n",
        "    self.preactivation = (self.weights @ input) + self.bias\n",
        "    self.activation = self.activation_function(self.preactivation)\n",
        "    return self.activation\n",
        "\n",
        "  def backwards(self, error_signal):\n",
        "    error_signal = error_signal * self.activation_function_backwards(self.preactivation) # dL/a * da/dpre\n",
        "    dLdW = np.outer(error_signal, self.input)       # to adjust the weights                                    dL/dpre * dpre/dW\n",
        "    dLdinput = error_signal @ self.weights # error signal for the next layer                          dL/dpre * dpre/dinput\n",
        "    return (dLdW, dLdinput)\n",
        "\n",
        "  def backwards_output(self, targets):\n",
        "    error_signal = cross_entropy_backwards(predictions=self.preactivation, targets=targets) #dL/dpre\n",
        "\n",
        "    dLdW = np.outer(error_signal, self.input)       # to adjust the weights                          dL/dpre * dpre/dW\n",
        "    dLdinput = self.weights.T @ error_signal # error signal for the next layer                dL/dpre * dpre/dinput\n",
        "    return (dLdW, dLdinput)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C5p_dRKHBxuM"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ANN():\n",
        "  def __init__(self):\n",
        "    self.layer1 = MLP_layer(32, 64, sigmoid, activation_function_backwards=sigmoid_backwards)\n",
        "    self.layer2 = MLP_layer(16, 32, sigmoid, activation_function_backwards=sigmoid_backwards)\n",
        "    self.layer3 = MLP_layer(10, 16, softmax)\n",
        "    self.layers = [self.layer1, self.layer2, self.layer3]\n",
        "\n",
        "    self.learning_rate = 0.03\n",
        "\n",
        "  def forward(self, input):\n",
        "    prediction = input\n",
        "    for layer in self.layers:\n",
        "      prediction = layer.forward(prediction)\n",
        "    return prediction\n",
        "\n",
        "\n",
        "  def backwards(self, targets):\n",
        "    (dLdW, error_signal) = self.layers[-1].backwards_output(targets)\n",
        "    self.layers[-1].weights -= self.learning_rate * dLdW\n",
        "    for layer in reversed(self.layers[:-1]):\n",
        "      (dLdW, error_signal) = layer.backwards(error_signal)\n",
        "      layer.weights -= self.learning_rate * dLdW\n",
        "\n"
      ],
      "metadata": {
        "id": "l2-oofTuKco5"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ann = ANN()\n",
        "\n",
        "\n",
        "# for _ in range(10):\n",
        "#   X_temp, y_temp = next(gen)\n",
        "#   print(X_temp, y_temp)\n",
        "#   show_img(X_temp)\n",
        "\n",
        "\n",
        "# X_temp, y_temp = next(gen)\n",
        "# prediction = ann.forward(X_temp)\n",
        "# loss = cross_entropy(prediction, y_temp)\n",
        "# print(f\"loss: {loss}\")\n",
        "# ann.backwards(y_temp)\n",
        "\n",
        "\n",
        "def training(ANN, epochs=100):\n",
        "  avg_losses = []\n",
        "\n",
        "  for i in range(epochs):\n",
        "    avg_loss = []\n",
        "    gen = shuffle_generator(X, y)\n",
        "    for (X_temp, y_temp) in gen:\n",
        "      prediction = ANN.forward(X_temp)\n",
        "      current_loss = cross_entropy(prediction, y_temp)\n",
        "      avg_loss.append(current_loss)\n",
        "      ANN.backwards(y_temp)\n",
        "    print(sum(avg_loss) / len(avg_loss))\n",
        "  return ann\n",
        "\n",
        "ann = training(ann)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOXyibKxlILm",
        "outputId": "99007a25-21fe-4f32-9c3a-64c519294594"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.2675142575777687\n",
            "2.1426873653964393\n",
            "2.0223607624069553\n",
            "1.9485592935479128\n",
            "1.8988268126387287\n",
            "1.867128062585637\n",
            "1.8405047936764274\n",
            "1.8207416494914448\n",
            "1.8026220465783058\n",
            "1.7838255051300052\n",
            "1.76171053581809\n",
            "1.7411156941849706\n",
            "1.723834838416441\n",
            "1.7043443413939023\n",
            "1.6909074211644122\n",
            "1.6792668346915178\n",
            "1.668316701386275\n",
            "1.6578133092799754\n",
            "1.647781204870292\n",
            "1.642434257091358\n",
            "1.6341225445444885\n",
            "1.62983343179348\n",
            "1.6225871891742285\n",
            "1.6177206102473114\n",
            "1.6133300480251644\n",
            "1.60931091763812\n",
            "1.6054817618001833\n",
            "1.6000078997514742\n",
            "1.5980970583590886\n",
            "1.5927229416847506\n",
            "1.5899142903614123\n",
            "1.5877691041731097\n",
            "1.5849051956163944\n",
            "1.5810831406444945\n",
            "1.5796226092074703\n",
            "1.5757616503905856\n",
            "1.573909090295955\n",
            "1.5697536588298246\n",
            "1.5695215196590493\n",
            "1.5665660086863185\n",
            "1.5651275294903906\n",
            "1.5614782696050433\n",
            "1.5607472459057945\n",
            "1.5582677452897784\n",
            "1.5569534839451353\n",
            "1.5552509287103617\n",
            "1.5534748146329052\n",
            "1.5516118492800302\n",
            "1.5505800627803155\n",
            "1.5483787203713104\n",
            "1.5464691473505796\n",
            "1.5466085454821936\n",
            "1.544787025319486\n",
            "1.5423170518835692\n",
            "1.5413931768692903\n",
            "1.5397719943220767\n",
            "1.5398112144892078\n",
            "1.537875494562871\n",
            "1.5362530380254384\n",
            "1.5362750818635131\n",
            "1.534727345155707\n",
            "1.5329353216961974\n",
            "1.5319481343992765\n",
            "1.5326696867786622\n",
            "1.5299562240114426\n",
            "1.5292825555985299\n",
            "1.5284245072167766\n",
            "1.5286599539569337\n",
            "1.5264500026458128\n",
            "1.5267081961242135\n",
            "1.5241806738680452\n",
            "1.5260453043127022\n",
            "1.5223563634387691\n",
            "1.5233932315373933\n",
            "1.5236400175741278\n",
            "1.520739723051853\n",
            "1.5213354803557568\n",
            "1.519188539650618\n",
            "1.520110989249391\n",
            "1.5173620209841132\n",
            "1.519370798552046\n",
            "1.5190205162481387\n",
            "1.5164391126258694\n",
            "1.516495610697754\n",
            "1.515625076398823\n",
            "1.5156284487968168\n",
            "1.5134401195526253\n",
            "1.514968722666607\n",
            "1.5145508151587377\n",
            "1.5119691203614343\n",
            "1.512671043125998\n",
            "1.512163711672981\n",
            "1.510517080562211\n",
            "1.5118857288229708\n",
            "1.5099343187991123\n",
            "1.5104902319825735\n",
            "1.5101826914486793\n",
            "1.5087165542685064\n",
            "1.5086498863270592\n",
            "1.508577178244223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gen = shuffle_generator(X, y)\n",
        "correct = 0\n",
        "total = 0\n",
        "for (X_temp, y_temp) in gen:\n",
        "  total += 1\n",
        "  prediction = ann.forward(X_temp)\n",
        "  if np.argmax(prediction) == np.argmax(y_temp):\n",
        "    correct += 1\n",
        "accuracy = correct/total\n",
        "print(f\"correct: {correct}, total: {total}, accuracy: {accuracy}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOBY2G2f_558",
        "outputId": "cfc7f781-7896-47c6-fc55-3ac7af2447bf"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "correct: 1784, total: 1797, accuracy: 0.9927657206455203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hHoYeb1RSPNW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}